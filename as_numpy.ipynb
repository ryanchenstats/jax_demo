{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX as Numpy\n",
    "\n",
    "JAX can be used as a drop-in replacement for numpy. There are other things that JAX can do, but jax.numpy can be used just as you would use numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random as jnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of x: <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "Sum of elements in x using dot(): 9993415.0\n",
      "Sum of elements in x using einsum(): 9993415.0\n",
      "Sum of elements in x using @: 9993415.0\n",
      "Operations result in equal values.\n"
     ]
    }
   ],
   "source": [
    "rng = jnr.key(42)\n",
    "x = jnr.normal(rng, (int(1e7)))\n",
    "y = jnr.normal(rng, (int(1e7)))\n",
    "\n",
    "print(\"Type of x:\", type(x))\n",
    "\n",
    "print(\"Sum of elements in x using dot():\", jnp.dot(x, y))\n",
    "\n",
    "print(\"Sum of elements in x using einsum():\", jnp.einsum('i,i', x, y))\n",
    "\n",
    "print(\"Sum of elements in x using @:\", x @ y)\n",
    "\n",
    "assert jnp.dot(x, y) == jnp.einsum('i,i', x, y) == x @ y\n",
    "print(\"Operations result in equal values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the counterpart in numpy and torch, and their out of box performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of nx: <class 'numpy.ndarray'>\n",
      "Type of yx: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "nx = np.array(x) # numpy can directly convert from jax\n",
    "tx = torch.tensor(nx) # torch cannot directly convert from jax\n",
    "\n",
    "ny = np.array(y)\n",
    "ty = torch.tensor(ny)\n",
    "\n",
    "print(\"Type of nx:\", type(nx))\n",
    "print(\"Type of yx:\", type(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 μs ± 22.2 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "703 μs ± 34.6 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "399 μs ± 14.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "3.34 ms ± 33.7 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.13 ms ± 54.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "136 μs ± 36.3 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jnp.dot(x, y).block_until_ready()\n",
    "%timeit jnp.einsum('i,i', x, y).block_until_ready()\n",
    "%timeit (x @ y).block_until_ready()\n",
    "\n",
    "%timeit nx @ ny\n",
    "%timeit tx @ ty\n",
    "tx = tx.to('cuda')\n",
    "ty = ty.to('cuda')\n",
    "%timeit tx @ ty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax is faster than numpy, in fact by a factor of about 25. This is due to GPU speed ups that JAX implements. We can see that JAX is also faster than torch if torch is on the CPU. When torch is on the GPU, the performance is similar.\n",
    "\n",
    "But we can speed up JAX even more with just in time compilation, in fact in this example, JAX is sped up by a factor of 4 more when using jit. This provides almost a 100x speedup compared to using numpy on CPU, and a 4x speedup compared to torch on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 μs ± 15.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def dot_product_examples(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "\n",
    "%timeit dot_product_examples(x, y).block_until_ready()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX has this particular feature of just in time compilation, which can speed up the performance of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
