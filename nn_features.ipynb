{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX for neural networks\n",
    "\n",
    "Jax, like torch, has a lot of features that are useful for neural networks, such as auto differentiation. Below, we look at the univariate function and evaluate its gradient at a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import grad\n",
    "from jax import random as jnr\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "grad_f = grad(f)\n",
    "grad_f(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A free extension of this is that we can find the linear approximation of the function at a given point. We can also apply the gradient of f to another gradient to get the second derivative. We can do this for any number of derivatives, allowing us to find higher order derivatives and compute Taylor expansions up to any finite order.\n",
    "\n",
    "Let's do the approximation of the square root function about x=43, granted we know that the square root of 49 is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 6.557438373565674, Approximate value: 6.571428298950195, Error: -0.013989925384521484\n"
     ]
    }
   ],
   "source": [
    "# First order approximation\n",
    "\n",
    "def sqrt(x):\n",
    "    return x**(1/2)\n",
    "\n",
    "d1 = grad(sqrt)\n",
    "\n",
    "# f(43 = 49 - 6) = sqrt(49) + d1(49)(-6)\n",
    "true_val = float(jnp.sqrt(43))\n",
    "approx_val = 7 + d1(49.0)*(-6)\n",
    "print(f\"True value: {true_val}, Approximate value: {approx_val}, Error: {true_val - approx_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the approximation changes with the order of the derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 6.557438373565674, Approximate value: 6.571428298950195, Error: -0.013989925384521484\n",
      "True value: 6.557438373565674, Approximate value: 6.545189380645752, Error: 0.012248992919921875\n",
      "True value: 6.557438373565674, Approximate value: 6.542779445648193, Error: 0.014658927917480469\n",
      "True value: 6.557438373565674, Approximate value: 6.5425333976745605, Error: 0.014904975891113281\n",
      "True value: 6.557438373565674, Approximate value: 6.542507171630859, Error: 0.014931201934814453\n"
     ]
    }
   ],
   "source": [
    "highest_order = 5\n",
    "d = sqrt\n",
    "approx_total = 7\n",
    "factorial = 1\n",
    "for i in range(1, highest_order+1):\n",
    "    d = grad(d)\n",
    "    approx_total += d(49.0)*((-6)**i) / factorial\n",
    "    factorial *= i\n",
    "\n",
    "    print(f\"True value: {float(jnp.sqrt(43))}, Approximate value: {approx_total}, Error: {true_val - approx_total}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can perform linear regression with JAX using the one layer neural network. This is full batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 283.57696533203125. Parameters: [0.53832483 1.2374917 ], -0.11600105464458466\n",
      "Epoch 100, Loss: 13.434804916381836. Parameters: [-9.487339 10.645889], 2.352383852005005\n",
      "Epoch 200, Loss: 9.301319122314453. Parameters: [-10.71081   11.789786], 2.786647319793701\n",
      "Epoch 300, Loss: 9.236555099487305. Parameters: [-10.860037  11.930452], 2.857307195663452\n",
      "Epoch 400, Loss: 9.235513687133789. Parameters: [-10.87819   11.947992], 2.8682963848114014\n",
      "Final weights: [-10.880371  11.950205], Final bias: 2.869945526123047\n",
      "Loss (variance estimate): 3.0389962196350098\n",
      "W_pvals: [0.00123334 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "key = jnr.PRNGKey(2)\n",
    "X1_key, X2_key = jnr.split(key)\n",
    "X1 = jnr.normal(X1_key, (1000, 1))\n",
    "X2 = jnr.normal(X2_key, (1000, 1))\n",
    "y = -11*X1 + 12*X2 + 3 + 3*jnr.normal(key, (1000, 1))\n",
    "\n",
    "X = jnp.hstack([X1, X2])\n",
    "\n",
    "def linear_model(in_size=2, out_size=1):\n",
    "    W_key, b_key = jnr.split(key)\n",
    "    w = jnr.normal(W_key, (in_size, out_size))\n",
    "    b = jnr.normal(b_key, (out_size,))\n",
    "    return w, b\n",
    "\n",
    "def predict(w, b, x): \n",
    "    return jnp.dot(x, w) + b\n",
    "\n",
    "def loss(w, b, x, y):\n",
    "    return jnp.mean((predict(w, b, x) - y)**2)\n",
    "\n",
    "W, b = linear_model(2, 1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch = 0   \n",
    "while True:\n",
    "    old_W, old_b = W, b\n",
    "    loss_val = loss(W, b, X, y)\n",
    "    grad_W, grad_b = grad(loss, (0, 1))(W, b, X, y)\n",
    "    W -= learning_rate * grad_W\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_val}. Parameters: {W.flatten()}, {b[0]}\")\n",
    "    \n",
    "    if jnp.linalg.norm(W-old_W) < 1e-5 and jnp.linalg.norm(b-old_b) < 1e-5:\n",
    "        break\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "\n",
    "# Final weights and bias\n",
    "print(f\"Final weights: {W.flatten()}, Final bias: {b[0]}\")\n",
    "print(f\"Loss (variance estimate): {jnp.sqrt(loss_val)}\")\n",
    "sigma2 = loss_val\n",
    "\n",
    "# With estimates, we can computer the p-values of the coefficients\n",
    "\n",
    "X_ = jnp.hstack([jnp.ones((1000, 1)), X]) # to do statistical inference on the bias\n",
    "W_ = jnp.hstack([b[0], W[0][0], W[1][0]])\n",
    "S = (1/sigma2)*jnp.dot(X_.T, X_)\n",
    "S = jnp.linalg.eigh(S)\n",
    "eig_vals, eig_vecs = S[0], S[1]\n",
    "W_std = sigma2 * eig_vecs @ jnp.diag(1/jnp.sqrt(eig_vals)) @ eig_vecs.T\n",
    "\n",
    "W_pvals = 2 * (1-norm.cdf(abs(W_.T) / jnp.diag(W_std)))\n",
    "print(f\"W_pvals: {W_pvals}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that everything is significant at the 5% level. Our variance estimate is exactly our final loss value.\n",
    "\n",
    "The parameters are very close to the true values of -11 and 12, and the bias is very close to 3.\n",
    "\n",
    "We can also do minibatch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9.741560935974121. Parameters: [-10.879044  11.950058], 2.869439125061035\n",
      "Epoch 10, Loss: 9.45537281036377. Parameters: [-10.878592   11.9523535], 2.8698081970214844\n",
      "Epoch 20, Loss: 8.84383773803711. Parameters: [-10.87631   11.952202], 2.8668839931488037\n",
      "Epoch 30, Loss: 8.962871551513672. Parameters: [-10.874984  11.951006], 2.8707189559936523\n",
      "Epoch 40, Loss: 9.642067909240723. Parameters: [-10.875083  11.957191], 2.8712234497070312\n",
      "Epoch 50, Loss: 9.332273483276367. Parameters: [-10.87588   11.960351], 2.870980739593506\n",
      "Epoch 60, Loss: 9.223517417907715. Parameters: [-10.876971  11.960799], 2.877840280532837\n",
      "Epoch 70, Loss: 8.820347785949707. Parameters: [-10.878455  11.961711], 2.879833936691284\n",
      "Epoch 80, Loss: 9.097527503967285. Parameters: [-10.876961  11.959031], 2.8802297115325928\n",
      "Epoch 90, Loss: 9.079719543457031. Parameters: [-10.876217  11.957327], 2.8783562183380127\n",
      "Epoch 100, Loss: 8.818466186523438. Parameters: [-10.873932  11.950064], 2.876361846923828\n",
      "Epoch 110, Loss: 8.80849838256836. Parameters: [-10.8715515  11.952709 ], 2.874678373336792\n",
      "Epoch 120, Loss: 8.984436988830566. Parameters: [-10.863605  11.955026], 2.876180648803711\n",
      "Epoch 130, Loss: 9.315001487731934. Parameters: [-10.867308  11.95569 ], 2.872105836868286\n",
      "Epoch 140, Loss: 9.967312812805176. Parameters: [-10.865894  11.954996], 2.8722472190856934\n",
      "Final weights: [-10.866315  11.954393], Final bias: 2.871422052383423\n",
      "Loss (variance estimate): 2.5346131324768066\n",
      "W_pvals: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_samples = X.shape[0]\n",
    "num_batches = 100  \n",
    "epoch = 0\n",
    "\n",
    "while True:\n",
    "    key = jnr.PRNGKey(epoch)\n",
    "    indices = jnr.randint(key=key, shape=(num_batches, batch_size), minval=0, maxval=num_samples)\n",
    "    minibatch_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        X_batch = X[indices[i]]\n",
    "        y_batch = y[indices[i]]\n",
    "        \n",
    "        old_W, old_b = W, b\n",
    "        loss_val = loss(W, b, X_batch, y_batch)\n",
    "        minibatch_loss += loss_val\n",
    "        grad_W, grad_b = grad(loss, (0, 1))(W, b, X_batch, y_batch)\n",
    "        W -= learning_rate * grad_W / batch_size\n",
    "        b -= learning_rate * grad_b / batch_size\n",
    "\n",
    "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f\"Epoch {epoch}, Loss: {minibatch_loss/num_batches}. Parameters: {W.flatten()}, {b[0]}\")\n",
    "    \n",
    "    if jnp.linalg.norm(W-old_W) < 1e-5 and jnp.linalg.norm(b-old_b) < 1e-5:\n",
    "        break\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "\n",
    "# Final weights and bias\n",
    "print(f\"Final weights: {W.flatten()}, Final bias: {b[0]}\")\n",
    "print(f\"Loss (variance estimate): {jnp.sqrt(loss_val)}\")\n",
    "sigma2 = loss_val\n",
    "\n",
    "# With estimates, we can computer the p-values of the coefficients\n",
    "\n",
    "X_ = jnp.hstack([jnp.ones((1000, 1)), X]) # to do statistical inference on the bias\n",
    "W_ = jnp.hstack([b[0], W[0][0], W[1][0]])\n",
    "S = (1/sigma2)*jnp.dot(X_.T, X_)\n",
    "S = jnp.linalg.eigh(S)\n",
    "eig_vals, eig_vecs = S[0], S[1]\n",
    "W_std = sigma2 * eig_vecs @ jnp.diag(1/jnp.sqrt(eig_vals)) @ eig_vecs.T\n",
    "\n",
    "W_pvals = 2 * (1-norm.cdf(abs(W_.T) / jnp.diag(W_std)))\n",
    "print(f\"W_pvals: {W_pvals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch gradient descent in this case is slower, but the estimated parameters are still very close to the true values. Furthermore, the convergence is less smooth, which is expected from the stochastic nature of the algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
