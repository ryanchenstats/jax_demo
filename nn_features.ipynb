{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX for neural networks\n",
    "\n",
    "Jax, like torch, has a lot of features that are useful for neural networks, such as auto differentiation. Below, we look at the univariate function and evaluate its gradient at a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import grad\n",
    "from jax import random as jnr\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "grad_f = grad(f)\n",
    "grad_f(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A free extension of this is that we can find the linear approximation of the function at a given point. We can also apply the gradient of f to another gradient to get the second derivative. We can do this for any number of derivatives, allowing us to find higher order derivatives and compute Taylor expansions up to any finite order.\n",
    "\n",
    "Let's do the approximation of the square root function about x=43, granted we know that the square root of 49 is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 6.557438373565674, Approximate value: 6.5714287757873535, Error: -0.013990402221679688\n"
     ]
    }
   ],
   "source": [
    "# First order approximation\n",
    "\n",
    "def sqrt(x):\n",
    "    return x**(1/2)\n",
    "\n",
    "d1 = grad(sqrt)\n",
    "\n",
    "# f(43 = 49 - 6) = sqrt(49) + d1(49)(-6)\n",
    "true_val = float(jnp.sqrt(43))\n",
    "approx_val = 7 + d1(49.0)*(-6)\n",
    "print(f\"True value: {true_val}, Approximate value: {approx_val}, Error: {true_val - approx_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the approximation changes with the order of the derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 6.557438373565674, Approximate value: 6.5714287757873535, Error: -0.013990402221679688\n",
      "True value: 6.557438373565674, Approximate value: 6.54518985748291, Error: 0.012248516082763672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 6.557438373565674, Approximate value: 6.542779922485352, Error: 0.014658451080322266\n",
      "True value: 6.557438373565674, Approximate value: 6.542533874511719, Error: 0.014904499053955078\n",
      "True value: 6.557438373565674, Approximate value: 6.542507648468018, Error: 0.01493072509765625\n"
     ]
    }
   ],
   "source": [
    "highest_order = 5\n",
    "d = sqrt\n",
    "approx_total = 7\n",
    "factorial = 1\n",
    "for i in range(1, highest_order+1):\n",
    "    d = grad(d)\n",
    "    approx_total += d(49.0)*((-6)**i) / factorial\n",
    "    factorial *= i\n",
    "\n",
    "    print(f\"True value: {float(jnp.sqrt(43))}, Approximate value: {approx_total}, Error: {true_val - approx_total}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can perform linear regression with JAX using the one layer neural network. This is full batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 283.57696533203125. Parameters: [0.53832495 1.2374916 ], -0.11600104719400406\n",
      "Epoch 100, Loss: 13.434798240661621. Parameters: [-9.48734 10.64589], 2.352383852005005\n",
      "Epoch 200, Loss: 9.301318168640137. Parameters: [-10.71081   11.789786], 2.786647319793701\n",
      "Epoch 300, Loss: 9.236554145812988. Parameters: [-10.860037  11.930452], 2.857306957244873\n",
      "Epoch 400, Loss: 9.235513687133789. Parameters: [-10.87819   11.947992], 2.8682963848114014\n",
      "Final weights: [-10.880371  11.950205], Final bias: 2.869945526123047\n",
      "Loss (variance estimate): 3.0389962196350098\n",
      "W_pvals: [0.00123334 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "key = jnr.PRNGKey(2)\n",
    "X1_key, X2_key = jnr.split(key)\n",
    "X1 = jnr.normal(X1_key, (1000, 1))\n",
    "X2 = jnr.normal(X2_key, (1000, 1))\n",
    "y = -11*X1 + 12*X2 + 3 + 3*jnr.normal(key, (1000, 1))\n",
    "\n",
    "X = jnp.hstack([X1, X2])\n",
    "\n",
    "def linear_model(in_size=2, out_size=1):\n",
    "    W_key, b_key = jnr.split(key)\n",
    "    w = jnr.normal(W_key, (in_size, out_size))\n",
    "    b = jnr.normal(b_key, (out_size,))\n",
    "    return w, b\n",
    "\n",
    "def predict(w, b, x): \n",
    "    return jnp.dot(x, w) + b\n",
    "\n",
    "def loss(w, b, x, y):\n",
    "    return jnp.mean((predict(w, b, x) - y)**2)\n",
    "\n",
    "W, b = linear_model(2, 1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch = 0   \n",
    "while True:\n",
    "    old_W, old_b = W, b\n",
    "    loss_val = loss(W, b, X, y)\n",
    "    grad_W, grad_b = grad(loss, (0, 1))(W, b, X, y)\n",
    "    W -= learning_rate * grad_W\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_val}. Parameters: {W.flatten()}, {b[0]}\")\n",
    "    \n",
    "    if jnp.linalg.norm(W-old_W) < 1e-5 and jnp.linalg.norm(b-old_b) < 1e-5:\n",
    "        break\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "\n",
    "# Final weights and bias\n",
    "print(f\"Final weights: {W.flatten()}, Final bias: {b[0]}\")\n",
    "print(f\"Loss (variance estimate): {jnp.sqrt(loss_val)}\")\n",
    "sigma2 = loss_val\n",
    "\n",
    "# With estimates, we can computer the p-values of the coefficients\n",
    "\n",
    "X_ = jnp.hstack([jnp.ones((1000, 1)), X]) # to do statistical inference on the bias\n",
    "W_ = jnp.hstack([b[0], W[0][0], W[1][0]])\n",
    "S = (1/sigma2)*jnp.dot(X_.T, X_)\n",
    "S = jnp.linalg.eigh(S)\n",
    "eig_vals, eig_vecs = S[0], S[1]\n",
    "W_std = sigma2 * eig_vecs @ jnp.diag(1/jnp.sqrt(eig_vals)) @ eig_vecs.T\n",
    "\n",
    "W_pvals = 2 * (1-norm.cdf(abs(W_.T) / jnp.diag(W_std)))\n",
    "print(f\"W_pvals: {W_pvals}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that everything is significant at the 5% level. Our variance estimate is exactly our final loss value.\n",
    "\n",
    "The parameters are very close to the true values of -11 and 12, and the bias is very close to 3.\n",
    "\n",
    "We can also do minibatch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9.73923110961914. Parameters: [-10.867279  11.956   ], 2.8681633472442627\n",
      "Epoch 10, Loss: 9.454119682312012. Parameters: [-10.868103  11.957705], 2.868723154067993\n",
      "Epoch 20, Loss: 8.845235824584961. Parameters: [-10.866956  11.957055], 2.8659415245056152\n",
      "Epoch 30, Loss: 8.964531898498535. Parameters: [-10.86666   11.955393], 2.8698999881744385\n",
      "Epoch 40, Loss: 9.640743255615234. Parameters: [-10.867653  11.96116 ], 2.8705267906188965\n",
      "Epoch 50, Loss: 9.333024978637695. Parameters: [-10.869248  11.963926], 2.870386838912964\n",
      "Epoch 60, Loss: 9.223723411560059. Parameters: [-10.871053  11.964029], 2.8773386478424072\n",
      "Epoch 70, Loss: 8.821696281433105. Parameters: [-10.873198  11.964625], 2.8794002532958984\n",
      "Epoch 80, Loss: 9.098153114318848. Parameters: [-10.8722725  11.961653 ], 2.8798582553863525\n",
      "Epoch 90, Loss: 9.080175399780273. Parameters: [-10.872027  11.959692], 2.8780481815338135\n",
      "Epoch 100, Loss: 8.819097518920898. Parameters: [-10.870214  11.952191], 2.8761041164398193\n",
      "Epoch 110, Loss: 8.808431625366211. Parameters: [-10.868257  11.954631], 2.874464511871338\n",
      "Epoch 120, Loss: 8.983665466308594. Parameters: [-10.860666  11.956765], 2.876006603240967\n",
      "Epoch 130, Loss: 9.315579414367676. Parameters: [-10.864679  11.95726 ], 2.8719592094421387\n",
      "Epoch 140, Loss: 9.96734619140625. Parameters: [-10.863527  11.956391], 2.872129440307617\n",
      "Final weights: [-10.863979  11.955773], Final bias: 2.8713057041168213\n",
      "Loss (variance estimate): 2.5345640182495117\n",
      "W_pvals: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_samples = X.shape[0]\n",
    "num_batches = 100  \n",
    "epoch = 0\n",
    "\n",
    "while True:\n",
    "    key = jnr.PRNGKey(epoch)\n",
    "    indices = jnr.randint(key=key, shape=(num_batches, batch_size), minval=0, maxval=num_samples)\n",
    "    minibatch_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        X_batch = X[indices[i]]\n",
    "        y_batch = y[indices[i]]\n",
    "        \n",
    "        old_W, old_b = W, b\n",
    "        loss_val = loss(W, b, X_batch, y_batch)\n",
    "        minibatch_loss += loss_val\n",
    "        grad_W, grad_b = grad(loss, (0, 1))(W, b, X_batch, y_batch)\n",
    "        W -= learning_rate * grad_W / batch_size\n",
    "        b -= learning_rate * grad_b / batch_size\n",
    "\n",
    "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f\"Epoch {epoch}, Loss: {minibatch_loss/num_batches}. Parameters: {W.flatten()}, {b[0]}\")\n",
    "    \n",
    "    if jnp.linalg.norm(W-old_W) < 1e-5 and jnp.linalg.norm(b-old_b) < 1e-5:\n",
    "        break\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "\n",
    "# Final weights and bias\n",
    "print(f\"Final weights: {W.flatten()}, Final bias: {b[0]}\")\n",
    "print(f\"Loss (variance estimate): {jnp.sqrt(loss_val)}\")\n",
    "sigma2 = loss_val\n",
    "\n",
    "# With estimates, we can computer the p-values of the coefficients\n",
    "\n",
    "X_ = jnp.hstack([jnp.ones((1000, 1)), X]) # to do statistical inference on the bias\n",
    "W_ = jnp.hstack([b[0], W[0][0], W[1][0]])\n",
    "S = (1/sigma2)*jnp.dot(X_.T, X_)\n",
    "S = jnp.linalg.eigh(S)\n",
    "eig_vals, eig_vecs = S[0], S[1]\n",
    "W_std = sigma2 * eig_vecs @ jnp.diag(1/jnp.sqrt(eig_vals)) @ eig_vecs.T\n",
    "\n",
    "W_pvals = 2 * (1-norm.cdf(abs(W_.T) / jnp.diag(W_std)))\n",
    "print(f\"W_pvals: {W_pvals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch gradient descent in this case is slower, but the estimated parameters are still very close to the true values. Furthermore, the convergence is less smooth, which is expected from the stochastic nature of the algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
