{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX for neural networks\n",
    "\n",
    "Jax, like torch, has a lot of features that are useful for neural networks, such as auto differentiation. Below, we look at the univariate function and evaluate its gradient at a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "grad_f = grad(f)\n",
    "grad_f(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A free extension of this is that we can find the linear approximation of the function at a given point. We can also apply the gradient of f to another gradient to get the second derivative. We can do this for any number of derivatives, allowing us to find higher order derivatives and compute Taylor expansions up to any finite order.\n",
    "\n",
    "Let's do the approximation of the square root function about x=43, granted we know that the square root of 49 is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 6.557438373565674, Approximate value: 6.5714287757873535, Error: -0.013990402221679688\n"
     ]
    }
   ],
   "source": [
    "# First order approximation\n",
    "\n",
    "def sqrt(x):\n",
    "    return x**(1/2)\n",
    "\n",
    "d1 = grad(sqrt)\n",
    "\n",
    "# f(43 = 49 - 6) = sqrt(49) + d1(49)(-6)\n",
    "true_val = float(jnp.sqrt(43))\n",
    "approx_val = 7 + d1(49.0)*(-6)\n",
    "print(f\"True value: {true_val}, Approximate value: {approx_val}, Error: {true_val - approx_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the approximation changes with the order of the derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 6.557438373565674, Approximate value: 6.5714287757873535, Error: -0.013990402221679688\n",
      "True value: 6.557438373565674, Approximate value: 6.54518985748291, Error: 0.012248516082763672\n",
      "True value: 6.557438373565674, Approximate value: 6.542779922485352, Error: 0.014658451080322266\n",
      "True value: 6.557438373565674, Approximate value: 6.542533874511719, Error: 0.014904499053955078\n",
      "True value: 6.557438373565674, Approximate value: 6.542507648468018, Error: 0.01493072509765625\n"
     ]
    }
   ],
   "source": [
    "highest_order = 5\n",
    "d = sqrt\n",
    "approx_total = 7\n",
    "factorial = 1\n",
    "for i in range(1, highest_order+1):\n",
    "    d = grad(d)\n",
    "    approx_total += d(49.0)*((-6)**i) / factorial\n",
    "    factorial *= i\n",
    "\n",
    "    print(f\"True value: {float(jnp.sqrt(43))}, Approximate value: {approx_total}, Error: {true_val - approx_total}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can perform linear regression with JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1301 steps to converge with loss 1.1249474442254215e-11 [ 0.999997  -0.4451895  3.1487293]\n",
      "[[0.9018224]\n",
      " [3.5300593]\n",
      " [1.7035173]]\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(42) \n",
    "X1 = random.normal(key, (1000,1))\n",
    "X2 = 3*random.normal(key, (1000,1))\n",
    "y = 3*X1 + 2*X2 + 1 + 0.001*random.normal(key, (1000,1))\n",
    "\n",
    "X0 = jnp.ones((1000,1))\n",
    "X = jnp.concatenate([X0, X1, X2], axis=-1)\n",
    "\n",
    "key = random.PRNGKey(43)\n",
    "b = random.normal(key, (3,))\n",
    "\n",
    "def loss(params_dict):\n",
    "    b0, b1, b2 = params_dict['b0'], params_dict['b1'], params_dict['b2']\n",
    "    yhat = b0 + b1 * X1 + b2 * X2\n",
    "    return jnp.mean((y - yhat)**2)\n",
    "\n",
    "counter = 0\n",
    "steps = 0\n",
    "while counter < 10:\n",
    "    loss_val = loss({'b0': b[0], 'b1': b[1], 'b2': b[2]})\n",
    "    dloss = grad(loss)({'b0': b[0], 'b1': b[1], 'b2': b[2]})\n",
    "    b_old = jnp.array([b[0], b[1], b[2]])\n",
    "    b0_new = b[0] - 0.005*dloss['b0']   \n",
    "    b1_new = b[1] - 0.005*dloss['b1']\n",
    "    b2_new = b[2] - 0.005*dloss['b2']\n",
    "    b = jnp.array([b0_new, b1_new, b2_new])\n",
    "    if jnp.linalg.norm(b - b_old) < 1e-27:\n",
    "        counter += 1\n",
    "    elif counter and jnp.linalg.norm(b - b_old) > 1e-5:\n",
    "        counter = 0\n",
    "    steps += 1\n",
    "print(f'Took {steps} steps to converge with loss {loss_val}', b)\n",
    "\n",
    "print(jnp.linalg.inv(X.T@X)@X.T@y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
